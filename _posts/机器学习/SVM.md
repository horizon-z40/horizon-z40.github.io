#### 1. SVM概念

​	特征空间上的间隔最大的线性分类器。

#### 2. 函数间隔与几何间隔

​	定义超平面$(w,b)$ 关于$(x_i, y_i)$ 的函数间隔为:
$$
\hat{\gamma} =y_i (w \cdot x_i + b)
$$
定义超平面关于训练数据集T的函数间隔为超平面$(w, b)$ 关于T中所有样本点$(x_i, y_i)$的的函数间隔的最小值。

函数间隔可以表示分类预测的正确性及确信度。

但是选择分离超平面时，只要成比例地改变$w$ 和 $b$ ，例如将它们改为$2w$ 和 $2b$ 超平面没有被改变，但是函数间隔却变为原来的的2倍。

可以对分离超平面加上某些约束，如规范化，使得间隔是确定的，这时函数间隔变成几何间隔。
$$
\gamma_i = \frac{w}{||w||} \cdot x_i + \frac{b}{||w||}
$$
其中$||w||$ 为$w$ 的L2范数。 

定义超平面关于训练数据集T的几何间隔为超平面$(w, b)$ 关于T中所有样本点$(x_i, y_i)$的的几何间隔的最小值。

#### 3. 最大间隔分离超平面

求一个几何间隔最大化地分离超平面，即最大间隔分离超平面，可以表示为下面的约束最优化问题：
$$
\max_{w, b}  \qquad  \gamma \\
s.t. \qquad y_i(\frac{w}{||w||}\cdot x_i + \frac{b}{||w||}) \ge \gamma
$$
即我们希望最大化超平面$(w,b)$ 关于训练数据集的几何间隔$\gamma$ 。

考虑到函数间隔和几何间隔的关系。上式可以写为：
$$
\max_{w,b} \qquad \frac{\hat{\gamma}}{||w||} \\
 s.t. \qquad y_i(w \cdot x_i + b) \ge \hat{\gamma}
$$
事实上，函数间隔$\hat{\gamma}$ 的取值并不影响最优化问题的解。成倍将w和b按比例地改变为$\lambda w$ 和$\lambda b$ ，这时函数间隔成为$\lambda \hat{\gamma}$ .函数间隔的这一改变没有影响。这样，可以取$\hat{\gamma}=1$ 。上面的最大化问题可以转换为：
$$
\min_{w,b}   \qquad \frac{1}{2} ||w||^2 \\
s.t. \qquad y_i(w\cdot x_i + b) - 1 \ge 0
$$
支持向量：

支持向量是使约束条件等号成立的点，即：
$$
y_i(w \cdot x_i +b) - 1 = 0
$$
支持向量的个数一般很少，所以支持向量机是由很少的“重要的”训练样本确定。

#### 4. 线性可分支持向量机

应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解。这样做的优点，一是对偶问题往往更容易求解，二是自然引入核函数，进而推广到非线性分类问题。

应用拉格朗日对偶性。原始问题的对偶问题是极大极小问题。
$$
\max_{\alpha} \min_{w,b} L(w, b, \alpha)
$$
对偶最优化问题：
$$
\min_{\alpha} \qquad \frac{1}{2} \sum_{i=1}^{N} \ \sum_{j=1}^N \alpha_i \alpha_j y_i y_j(x_i \cdot x_j) - \sum_{i=1}^N \alpha_i \\
s.t. \qquad \sum_{i=1}^N \alpha_iy_i = 0
$$

#### 5. 线性支持向量机

#### 6. 核技巧

#### 7. 非线性支持向量机

#### 8. 序列最小最优化算法

#### 9. SVM的损失函数