#### 1. KL距离

KL距离，是Kullback-Leibler差异（Kullback-Leibler Divergence）的简称，也叫做相对熵（Relative Entropy）。它衡量的是相同事件空间里的**两个概率分布的差异**情况。

其物理意义是：在相同事件空间里，概率分布P(x)对应的每个事件，若用概率分布 Q(x)编码时，平均每个基本事件（符号）编码长度增加了多少比特。我们用$D（P||Q）$表示KL距离，计算公式如下：
$$
D(P||Q) = \sum_{x \in X} P(x)log{\frac{P(x)}{Q(x)}}
$$


 当两个概率分布完全相同时，即P(X)=Q(X)，其相对熵为0 。我们知道，概率分布P(X)的信息熵为：
$$
H(P) = - \sum_{x \in X} P(x) log {P(x)}
$$
  其表示，概率分布P(x)编码时，平均每个基本事件（符号）至少需要多少比特编码。

通过信息熵的学习，我们知道不存在其他比按照本身概率分布更好的编码方式了，所以D(P||Q）始终大于等于0的。虽然KL被称为距离，但是其不满足距离定义的三个条件：1）非负性（满足）；2）对称性（不满足）；3）三角不等式 （不满足）。



百度百科解释的为什么KL距离不准确，不满足距离的概念：

①KL散度不对称，即P到Q的距离，不等于Q到P的距离

②KL散度不满足三角距离公式，两边之和大于第三边，两边之差小于第三边。



#### 2. 交叉熵

$$
H(p,q)＝E_p[-\text{log}q] = H(p) + D_{KL}(p||q)
$$

其中$H(p)$ 是$p$ 的熵。

对于离散分布$p$ 和$q$ ，这意味着：
$$
H(p, q) = - \sum_{x} p(x) \text{log}q(x)
$$


#### 3. 信息度量

信息论中，把信息大小解释为其不确定度。如果一个事件必然发生，那么他没有不确定度，也就不包含信息。即信息=不确定度。

