#### 1. BGD和SGD和MiniBatch-SGD

##### 1.1 BGD

$$
\Theta = \Theta - \alpha \cdot \nabla_{\Theta}J(\Theta)
$$

优点：

- cost fuction若为凸函数，能够保证收敛到全局最优值；若为非凸函数，能够收敛到局部最优值

缺点：

- 由于每轮迭代都需要在整个数据集上计算一次，所以批量梯度下降可能非常慢
- 训练数较多时，需要较大内存
- 批量梯度下降不允许在线更新模型，例如新增实例。

##### 1.2 SGD

和批梯度下降算法相反，SGD算法每读入一个数据，便立刻计算cost fuction的梯度来更新参数： 
$$
\Theta = \Theta - \alpha \cdot \nabla_{\Theta}J(\Theta;x^{(i)},y^{(i)})
$$
优点：

- 算法收敛速度快（BGD中，每轮会计算很多相似样本的梯度，这部分是冗余的）
- 可以在线更新
- 有几率跳出局部最优点

##### 1.3 MiniBatch-SGD

$$
\Theta = \Theta - \alpha \cdot \nabla_{\Theta}J(\Theta;x^{(i:i+n)},y^{(i:i+n)})
$$

MiniBatch-SGD 是上述两个方法的折中，在每轮迭代中仅计算一个miniBatch的梯度，计算效率高，收敛更稳定。

总的来说，上述三个方法面临的主要挑战：

- 选择适当的学习率$\alpha$ 较为困难，太小的学习率会导致收敛缓慢，太大则会造成波动，导致无法收敛。
- 每个参数的学习率相同，是不合理的。

#### 3. 牛顿法



#### 4. 拟牛顿法



#### 5. 动量(Momentum)

​	模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力：
$$
v_t= \gamma \cdot v_{t-1} + \alpha \cdot \nabla_{\Theta}J(\Theta) \\
\Theta = \Theta - v_t
$$
Momentum算法会观察历史梯度$v_{t-1}$， 若当前梯度的方向与历史梯度一致，则会增强这个方向的梯度，如不一致，则梯度会衰减。

#### 6. Nesterov梯度加速法(NAG)

在小球向下滚动的过程中，我们希望小球能够提前知道在哪些地方坡面会上升，这样在遇到上升坡面之前，小球就开始减速。这方法就是Nesterov Momentum，其在凸优化中有较强的理论保证收敛。并且，在实践中Nesterov Momentum也比单纯的 Momentum 的效果好：
$$
v_t = \gamma \cdot v_{t-1} + \alpha \cdot \nabla_{\Theta} J(\Theta- \gamma\cdot v_{t-1})\\
\Theta = \Theta - v_t
$$
​	注意到 momentum 方法，如果只看 $γ \cdot v_{t-1}$ 项，那么当前的 $\Theta$ 经过 momentum 的作用会变成 $\Theta-\gamma \cdot v_{t-1}$。因此可以把 $\Theta+\gamma \cdot v_{t-1}$这个位置看做是当前优化的一个”展望”位置。所以，可以在 $\Theta+\gamma \cdot v_{t-1}$求导, 而不是原始的$\Theta$。 

#### 7. AdaGrad

Adagrad方法是通过参数来调整合适的学习率η，**对稀疏参数进行大幅更新和对频繁参数进行小幅更新**。因此，Adagrad方法非常适合处理稀疏数据。**Adagrad方法是在每个时间步中，根据过往已计算的参数梯度，来为每个参数θ(i)修改对应的学习率η。**

我们设$g_{t,i}$ 为第t轮第i个参数的梯度，即$g_{t,i}=\nabla_{\Theta}J(\Theta_i)$ ，因此SGD中参数更新过程可写为：
$$
\Theta_{t+1, i} = \Theta_{t,i} - \alpha \cdot g_{t,i}
$$
Adagrad在每轮训练中，对每个参数$\theta_i$ 的学习率进行更新，参数更新公示如下：
$$
\Theta_{t+1, i} = \Theta_{t,i} - \frac{\alpha}{\sqrt{G_{t,ii}+\epsilon}}\cdot g_{t,i}
$$
其中，$G_t \in \mathbb{R}^{d \times d}$ 为对角矩阵，每个对角线位置(i,i) 为对应参数$\theta_i$ 从第1轮到第t轮梯度的平方和。

Adagrad方法的主要好处是，不需要手工来调整学习率。大多数参数使用了默认值0.01，且保持不变。

**Adagrad方法的主要缺点是，学习率η总是在降低和衰减。**因为每个附加项都是正的，在分母中累积了多个平方梯度值，故累积的总和在训练期间保持增长。这反过来又导致学习率下降，变为很小数量级的数字，该模型完全停止学习，停止获取新的额外知识。

因为随着学习速度的越来越小，模型的学习能力迅速降低，而且收敛速度非常慢，需要很长的训练和学习，即学习速度降低。

另一个叫做Adadelta的算法改善了这个学习率不断衰减的问题。

#### 8. Adadelta

这是一个AdaGrad的延伸方法，它倾向于解决其学习率衰减的问题。**Adadelta不是累积所有之前的平方梯度，而是将累积之前梯度的窗口限制到某个固定大小w。**

AdaDelta方法的另一个优点是，已经不需要设置一个默认的学习率。

目前已完成的改进

**1) 为每个参数计算出不同学习率；**

**2) 也计算了动量项momentum；**

3) 防止学习率衰减或梯度消失等问题的出现。

还可以做什么改进？

在之前的方法中计算了每个参数的对应学习率，但是**为什么不计算每个参数的对应动量变化并独立存储呢**？这就是Adam算法提出的改良点。

#### 9. Adam

Adam算法即自适应时刻估计方法（Adaptive Moment Estimation），它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。这个方法不仅存储了AdaDelta先前平方梯度的指数衰减平均值，而且保持了先前梯度M(t)的指数衰减平均值，这一点与动量类似：

$$
m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t \\
v_t = \beta_1 v_{t-1} + (1-\beta_1)g_t^2 \\
\hat{m_t} = \frac{m_t}{1-\beta_1^t} \\
\hat{v_t} = \frac{v_t}{1-\beta_2^t}
$$
$m_t, v_t$ 分别是对梯度的一阶矩估计和二阶矩估计，可以看做对期望$E[g_t]$ 与 $E[g_t^2]$ 的近似；$\hat{m}_t, \hat{v_t}$ 是对$m_t$ 和$v_t$ 的校正，这样可以近似为对期望的无偏估计。Adam算法的提出者建议$\beta_1$ 的默认值为0.9，$\beta_2$ 的默认值为0.999。

则参数更新的最终公式为：
$$
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{\hat{v_t}}+\epsilon} \hat{m_t}
$$
Adam 算法的提出者描述其为两种随机梯度下降扩展式的优点集合，即：

- 适应性梯度算法（AdaGrad）为每一个参数保留一个学习率以提升在稀疏梯度（即自然语言和计算机视觉问题）上的性能。

- 均方根传播（RMSProp）基于权重梯度最近量级的均值为每一个参数适应性地保留学习率。这意味着算法在非稳态和在线问题上有很有优秀的性能。

Adam 算法同时获得了 AdaGrad 和 RMSProp 算法的优点。Adam 不仅如 RMSProp 算法那样基于一阶矩均值计算适应性参数学习率，它同时还充分利用了梯度的二阶矩均值（即有偏方差/uncentered variance）。具体来说，算法计算了梯度的指数移动均值（exponential moving average），超参数 beta1 和 beta2 控制了这些移动均值的衰减率。

alpha：同样也称为学习率或步长因子，它控制了权重的更新比率（如 0.001）。较大的值（如 0.3）在学习率更新前会有更快的初始学习，而较小的值（如 1.0E-5）会令训练收敛到更好的性能。

beta1：一阶矩估计的指数衰减率（如 0.9）。

beta2：二阶矩估计的指数衰减率（如 0.999）。该超参数在稀疏梯度（如在 NLP 或计算机视觉任务中）中应该设置为接近 1 的数。

epsilon：该参数是非常小的数，其为了防止在实现中除以零（如 10E-8）。

#### 10. RMSProp

在 Adagrad 中， v_t]是单调递增的，使得学习率逐渐递减至 0，可能导致训练过程提前结束。为了改进这一缺点，可以考虑在计算二阶动量时不累积全部历史梯度，而只关注最近某一时间窗口内的下降梯度。根据此思想有了 RMSprop。

​	Adagrad会累加之前所有的梯度平方，而RMSprop仅仅是计算对应的平均值，因此可缓解Adagrad算法学习率下降较快的问题。 
$$
E[g^2]_t = \rho *E[g^2]_{t-1} + (1-\rho)* g_t^2 \\
\Theta_{t+1} = \Theta_{t} - \frac{\alpha}{\sqrt{E[|g^2]_t}+\epsilon}\cdot g_t
$$
**特点：**

- 其实RMSprop依然依赖于全局学习率
- RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间
- 适合处理非平稳目标 - 对于RNN效果很好

