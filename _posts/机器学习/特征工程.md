- 分箱: 将数值型属性转换成类别呈现更有意义，同时能使算法减少噪声的干扰，通过将一定范围内的数值划分成确定的块。

- 交叉特征：交叉特征算是特征工程中非常重要的方法之一了，交叉特征是一种很独特的方式，它将两个或更多的类别属性组合成一个。当组合的特征要比单个特征更好时，这是一项非常有用的技术。数学上来说，是对类别特征的所有可能值进行交叉相乘。

- 特征组合（特征交叉）

  - 将一个特征与其本身或其他特征相乘（称为特征组合）。

  - 两个特征相除。

  - 对连续特征进行分桶，以分为多个区间分箱。

  - 通过将单独的特征进行组合（相乘或求笛卡尔积）而形成的合成特征。特征组合有助于表示非线性关系。

    > 笛卡尔积：两个集合X和Y的笛卡尓积（Cartesian product），又称直积表示为X × Y，第一个对象是X的成员而第二个对象是Y的所有可能有序对的其中一个成员。
    > 假设集合A={a, b}，集合B={0, 1, 2}，则两个集合的笛卡尔积为{(a, 0), (a, 1), (a, 2), (b, 0), (b, 1), (b, 2)}。

  - 



#### 2. RF、GBDT、Xgboost特征选择的方法

- RF

具体的方法就是：
1. 对于每一棵决策树，用OOB 计算袋外数据误差，记为 errOOB1；
2. 然后随机对OOB所有样本的特征i加入噪声干扰，再次计算袋外数据误差，记为errOOB2；
3. 假设有N棵树，特征i的重要性为$\frac{\sum(errOOB2-errOOB1)}{N}​$ 
    如果加入随机噪声后，袋外数据准确率大幅下降，说明这个特征对预测结果有很大的影响，进而说明它的重要程度比较高

- GBDT

主要是通过计算特征i在单棵树中重要度的平均值，计算公式如下：
$$
\hat{J}_j^2 = \frac{1}{M} \sum_{m=1}^M \hat{J}_j^2(T_m)
$$
其中，M是树的数量。特征i在单棵树中的重要度是通过计算按这个特征i分裂之后损失的减少值。其中,L是叶子节点的数量，L-1就是非叶子结点的数量。

- XGBOOST

  XGboost是通过该特征每棵树中分裂次数的和去计算的，比如这个特征在第一棵树分裂1次，第二棵树2次……，那么这个特征的得分就是(1+2+...)。
