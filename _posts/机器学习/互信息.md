##### 1. 概念

互信息，Mutual Information，缩写为MI，表示两个变量X与Y是否有关系，以及关系的强弱。不同于相关系数，互信息并不局限于实值随机变量，它更加一般且决定着联合分布 p(X,Y) 和分解的边缘分布的乘积 p(X)p(Y) 的相似程度。

直观上，互信息度量 X 和 Y 共享的信息：它度量知道这两个变量其中一个，对另一个不确定度减少的程度、。

如果 X 和 Y 相互独立，则知道 X 不对 Y 提供任何信息，反之亦然，所以它们的互信息为零。在另一个极端，如果 X 是 Y 的一个确定性函数，且 Y 也是 X 的一个确定性函数，那么传递的所有信息被 X 和 Y 共享：知道 X 决定 Y 的值，反之亦然。

##### 2. 公式

$$
I(X;Y) = \sum_{y \in Y} \sum_{x \in X} p(x,y)log(\frac{p(x,y)}{p(x)p(y)})
$$

其中，$p(x,y)$ 是X和Y的联合概率分布函数，而p(x)和p(y)分别是X和Y的边缘概率分布函数。

